#!/bin/bash
# export CUDA_VISIBLE_DEVICES='0,3'
# uv run bash cs336_alignment/run_grpo_offpolicy.sh
# ================= 1. åŸºç¡€è·¯å¾„ä¸é¡¹ç›®é…ç½® =================
BASE_MODEL="model/Qwen2.5-Math-1.5B" 

# TRAIN_DATA="data/gsm8k/train.jsonl"
# VAL_DATA="data/gsm8k/test.jsonl"
# WANDB_PROJECT="cs336-grpo-after-base-offpolicy"

TRAIN_DATA="data/math12k/data/train-00000-of-00001.parquet"
VAL_DATA="data/math12k/data/test-00000-of-00001.parquet"
WANDB_PROJECT="cs336-grpo-math12k-after-base-offpolicy"

PROMPT_TEMPLATE="cs336_alignment/prompts/r1_zero.prompt"
OUTPUT_BASE="result/grpo_offpolicy_study"


# ================= 2. å­¦ä¹ ç‡é”šç‚¹é€»è¾‘ =================
# é”šç‚¹ï¼š1ä¸ª Epoch, 256 Batch æ—¶çš„åŸºå‡†å­¦ä¹ ç‡
ANCHOR_LR="0.00003"
ANCHOR_BATCH=256

# ================= 3. å®šä¹‰å…·ä½“çš„å®éªŒé…ç½®åˆ—è¡¨ =================
# æ ¼å¼ä¸º "Epochs:TrainBatchSize"
CONFIGS=(
    "1:256"   
    "1:64"    
    "1:128"  
    "3:64" 
    "3:128" 
    "3:256"    
    "2:64"    
    "2:256"   
    "2:128"  
)

# ================= 4. ç¡¬ä»¶ä¸é€šç”¨è¶…å‚ =================
MICRO_BS=8        # ç‰©ç†æ˜¾å­˜ Batch Size
ROLLOUT_SIZE=256   # é‡‡æ ·æ€»æ•°
GROUP_SIZE=8       # G
N_STEPS=200        # æ€»è¿­ä»£æ­¥æ•°
SEED=42

# ================= 5. å¾ªç¯æ‰§è¡Œå®éªŒ =================
TOTAL_EXPS=${#CONFIGS[@]}
CURR_EXP=0

for CFG in "${CONFIGS[@]}"; do
    ((CURR_EXP++))

    # è§£æé…ç½®ï¼šå°† "3:128" åˆ†è§£ä¸º E=3, TB=128
    IFS=":" read -r E TB <<< "$CFG"

    # --- å…³é”®é€»è¾‘ï¼šåŠ¨æ€è®¡ç®—è¶…å‚æ•° ---
    # 1. è®¡ç®—å­¦ä¹ ç‡: LR = Anchor_LR * (1/sqrt(E)) * (TB / Anchor_Batch)
    LR=$(awk "BEGIN {print $ANCHOR_LR * (1/sqrt($E)) * ($TB/$ANCHOR_BATCH)}")
    
    # 2. è®¡ç®—æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: AccumSteps = TB / MicroBS
    ACCUM_STEPS=$((TB / MICRO_BS))
    
    # 3. æ„é€ è¿è¡Œåç§°
    RUN_NAME="E${E}_TB${TB}_LR${LR}_no-std"
    CURRENT_OUTPUT_DIR="${OUTPUT_BASE}/${RUN_NAME}"

    echo "========================================================="
    echo "ğŸš€ [è¿›åº¦ $CURR_EXP/$TOTAL_EXPS] å¯åŠ¨å®éªŒ: $RUN_NAME"
    echo "ğŸ“Š é…ç½®: Epochs=$E | TrainBatch=$TB | AccumSteps=$ACCUM_STEPS"
    echo "ğŸ“ˆ è®¡ç®—å¾—å‡ºå­¦ä¹ ç‡: $LR"
    echo "ğŸ“‚ ä¿å­˜ç›®å½•: $CURRENT_OUTPUT_DIR"
    echo "========================================================="

    # æ‰§è¡Œè®­ç»ƒ
    uv run python cs336_alignment/train_grpo.py \
        --model_id "$BASE_MODEL" \
        --train_data_path "$TRAIN_DATA" \
        --test_data_path "$VAL_DATA" \
        --prompt_path "$PROMPT_TEMPLATE" \
        --output_dir "$CURRENT_OUTPUT_DIR" \
        --n_grpo_steps "$N_STEPS" \
        --lr "$LR" \
        --rollout_batch_size "$ROLLOUT_SIZE" \
        --group_size "$GROUP_SIZE" \
        --train_batch_size "$TB" \
        --gradient_accumulation_steps "$ACCUM_STEPS" \
        --epochs_per_rollout_batch "$E" \
        --loss_type "grpo_clip" \
        --length_norm_type "mask_normalize" \
        --device cuda:0 \
        --vllm_device cuda:1 \
        --vllm_gpu_util 0.3 \
        --eval_every_steps 8 \
        --wandb_project "$WANDB_PROJECT" \
        --wandb_run_name "$RUN_NAME" \
        --seed "$SEED" \
        # --use_std_normalization # é»˜è®¤å¼€å¯æ ‡å‡†å·®å½’ä¸€åŒ–

    # é”™è¯¯å¤„ç†
    if [ $? -ne 0 ]; then
        echo "âŒ å®éªŒ $RUN_NAME å¤±è´¥ï¼Œåœæ­¢åç»­è„šæœ¬ã€‚"
        exit 1
    fi
    
    echo "âœ… å®éªŒ $RUN_NAME å®Œæˆï¼"
    echo "---------------------------------------------------------"
    sleep 10 # ç¼“å†²ï¼Œç¡®ä¿ vLLM é‡Šæ”¾æ˜¾å­˜
done

echo "ğŸ‰ é¢„å®šä¹‰å®éªŒåˆ—è¡¨å…¨éƒ¨æ‰§è¡Œå®Œæ¯•ï¼"