import torch
import json
import random
import wandb
import os
import argparse
import pandas as pd
import numpy as np
from tqdm import tqdm
from torch.optim import AdamW
from transformers import AutoModelForCausalLM, AutoTokenizer
from vllm import LLM, SamplingParams
from unittest.mock import patch
import pandas as pd

# --- å¯¼å…¥è‡ªå®šä¹‰å·¥å…·å‡½æ•° ---
from cs336_alignment.sft_utils import (
    tokenize_prompt_and_output, 
    sft_microbatch_train_step,
    log_generations,
    compute_entropy
)
from cs336_alignment.drgrpo_grader import r1_zero_reward_fn, question_only_reward_fn

# ==========================================
# è¾…åŠ©å‡½æ•°
# ==========================================

def init_vllm(model_id, device, seed, gpu_memory_utilization):
    """åˆå§‹åŒ– vLLM å®ä¾‹ç”¨äºæ¨ç†è¯„ä¼°å’Œæ•°æ®ç”Ÿæˆã€‚"""
    with patch("torch.distributed.get_world_size", return_value=1), \
         patch("vllm.worker.worker.Worker._assert_memory_footprint_increased_during_profiling", return_value=None):
        return LLM(
            model=model_id, 
            device=device, 
            dtype=torch.bfloat16,
            enable_prefix_caching=True, 
            gpu_memory_utilization=gpu_memory_utilization,
            seed=seed
        )

def load_policy_into_vllm_instance(policy, llm):
    """å°†è®­ç»ƒä¸­çš„ PyTorch æ¨¡å‹æƒé‡åŒæ­¥åˆ° vLLMã€‚"""
    state_dict = policy.state_dict()
    llm_model = llm.llm_engine.model_executor.driver_worker.model_runner.model
    llm_model.load_weights(state_dict.items())
    print("\n[Sync] Policy weights synced to vLLM.")

def get_batch(tokenized_data, batch_size, device):
    """ä»ä¸“å®¶æ•°æ®ä¸­é‡‡æ · Batchï¼Œæ”¯æŒé‡å¤é‡‡æ ·ä»¥ä¿è¯ Step é€»è¾‘ã€‚"""
    total_len = len(tokenized_data["input_ids"])
    # ä½¿ç”¨ np.random.choice ç¡®ä¿å³ä½¿æ•°æ®æå°‘ä¹Ÿèƒ½å‡‘å¤Ÿä¸€ä¸ª Batch
    batch_indices = np.random.choice(total_len, batch_size, replace=True)
    
    return {
        "input_ids": tokenized_data["input_ids"][batch_indices].to(device),
        "labels": tokenized_data["labels"][batch_indices].to(device),
        "response_mask": tokenized_data["response_mask"][batch_indices].to(device)
    }

def load_math12k_dataset(path, prompt_template=None):
    df = pd.read_parquet(path)
    processed_items = []
    for _, row in df.iterrows():
        q_text = row['problem']
        gold_answer = row['answer']
        
        if gold_answer:
            processed_items.append({
                "prompt": prompt_template.replace("{question}", q_text),
                "gold": gold_answer
            })
    return processed_items

def load_gsm8k_dataset(path, prompt_template=None):
    
    processed_items = []
    with open(path, "r") as f:
        for line in f:
            item = json.loads(line)
            q_text = item['question']
            full_sol = item['answer']
            gold_answer = full_sol.split("####")[-1].strip() if "####" in full_sol else full_sol.strip()
            processed_items.append({
                "prompt": prompt_template.replace("{question}", q_text),
                "gold": gold_answer
            })
    return processed_items


# ==========================================
# Expert Iteration æ ¸å¿ƒè®­ç»ƒé€»è¾‘
# ==========================================

def run_expert_iteration(args):
    # 1. åŸºç¡€é…ç½®
    grad_accum_steps = args.batch_size // args.micro_batch_size
    
    wandb.init(project=args.wandb_project, name=args.wandb_run_name, config=vars(args))
    if 'r1' in args.prompt_path.lower():
        print("ä½¿ç”¨ R1 è¯„ä¼°æ¨¡ç‰ˆ (é›¶å¥–åŠ±å‡½æ•°)")
        reward_fn = r1_zero_reward_fn
    elif 'question_only' in args.prompt_path.lower():
        print("ä½¿ç”¨ Question-Only è¯„ä¼°æ¨¡ç‰ˆ")
        reward_fn = question_only_reward_fn
    else:
        raise ValueError("æ— æ³•è¯†åˆ«çš„è¯„ä¼°æ¨¡ç‰ˆï¼Œè¯·ç¡®ä¿ prompt_path ä¸­åŒ…å« 'r1' æˆ– 'question_only' ä»¥é€‰æ‹©å¯¹åº”çš„å¥–åŠ±å‡½æ•°ã€‚")


    # å®šä¹‰ WandB åæ ‡è½´
    wandb.define_metric("global_step")
    wandb.define_metric("ei_step")
    wandb.define_metric("train/*", step_metric="global_step")
    wandb.define_metric("eval/*", step_metric="global_step")


    # 2. æ¨¡å‹ä¸åˆ†è¯å™¨åˆå§‹åŒ–
    print(f"Initializing Model: {args.model_id}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_id)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    policy = AutoModelForCausalLM.from_pretrained(
        args.model_id, 
        torch_dtype=torch.bfloat16, 
        low_cpu_mem_usage=True,
        attn_implementation="flash_attention_2"
    ).to(args.device)
    policy.gradient_checkpointing_enable()
    
    optimizer = AdamW(policy.parameters(), lr=args.lr)
    amp_ctx = torch.amp.autocast(device_type="cuda", dtype=torch.bfloat16)

    print(f"Initializing vLLM on {args.vllm_device}...")
    vllm_inst = init_vllm(args.model_id, args.vllm_device, args.seed, args.vllm_gpu_util)


    with open(args.prompt_path, "r") as f:
        prompt_template = f.read().strip()
    # 3. æ•°æ®æ± ä¸éªŒè¯é›†å‡†å¤‡
    print("Loading data pools...")
    if 'math12k' in args.train_data_path.lower():
        question_pool = load_math12k_dataset(args.train_data_path, prompt_template)
        val_samples = load_math12k_dataset(args.val_data_path, prompt_template)[:args.max_eval_samples]
    elif 'gsm8k' in args.train_data_path.lower():
        question_pool = load_gsm8k_dataset(args.train_data_path, prompt_template)
        val_samples = load_gsm8k_dataset(args.val_data_path, prompt_template)[:args.max_eval_samples]
    else:
        raise ValueError("Unsupported dataset. Please use Math12K or GSM8K.")
    
    eval_params = SamplingParams(temperature=0.0, max_tokens=args.max_tokens, stop=["</answer>"], include_stop_str_in_output=True)
    rollout_params = SamplingParams(n=args.rollouts, temperature=1.0, max_tokens=args.max_tokens, min_tokens=4, stop=["</answer>"], include_stop_str_in_output=True)

    # ----------------------------------------------------------
    # 4. Step 0 åˆå§‹è¯„ä¼° (Baseline)
    # ----------------------------------------------------------
    print(f"\n[Step 0] æ‰§è¡Œè®­ç»ƒå‰åˆå§‹è¯„ä¼°...")
    policy.eval()
    load_policy_into_vllm_instance(policy, vllm_inst)
    metrics = log_generations(vllm_inst, eval_params, 
                             [s['prompt'] for s in val_samples], 
                             [s['gold'] for s in val_samples], 
                             reward_fn, 0, "eval")
    print(f"Initial Accuracy: {metrics.get('eval/accuracy', 0):.2%}")

    # ----------------------------------------------------------
    # 5. Expert Iteration ä¸»å¾ªç¯
    # ----------------------------------------------------------
    global_optim_step = 0

    for ei_step in range(args.n_ei_steps):
        print(f"\n{'='*20} å¼€å§‹ EI ç¬¬ {ei_step + 1} ä»£æ¼”åŒ– {'='*20}")
        
        # --- A. é‡‡æ ·é˜¶æ®µ (Rollout) ---
        policy.eval()
        load_policy_into_vllm_instance(policy, vllm_inst)
        
        # ä»æ± å­ä¸­éšæœºé€‰é¢˜ç›®è¿›è¡Œâ€œè€ƒè¯•â€
        batch_db = random.sample(question_pool, min(args.ei_batch_size, len(question_pool)))
        print(f">> æ­£åœ¨å¯¹ {len(batch_db)} ä¸ªé—®é¢˜è¿›è¡Œé‡‡æ · (G={args.rollouts})...")
        outputs = vllm_inst.generate([q['prompt'] for q in batch_db], rollout_params)
        
        # --- B. è¿‡æ»¤é˜¶æ®µ (Verify) ---
        expert_raw_data = []
        success_question_num = 0
        for i, output in enumerate(outputs):
            current_gold = batch_db[i]['gold']
            success_flag = 0
            for candidate in output.outputs:
                # åªæœ‰æ ¼å¼å’Œç­”æ¡ˆåŒå¯¹çš„æ‰ä¿ç•™
                if reward_fn(candidate.text, current_gold)['reward'] == 1.0:
                    success_flag = 1
                    expert_raw_data.append({"prompt": batch_db[i]['prompt'], "response": candidate.text})
            success_question_num += success_flag
        
        success_rate = len(expert_raw_data) / (len(batch_db) * args.rollouts)
        
        print(f">> é‡‡æ ·æˆåŠŸç‡: {success_rate:.2%} | è·å¾—ä¸“å®¶æ ·æœ¬: {len(expert_raw_data)}")
        wandb.log({"ei/success_rate": success_rate,
                    "ei/success_question_rate": success_question_num/len(batch_db),
                     "ei/collected_count": len(expert_raw_data),
                      "ei_step": ei_step + 1}, step=global_optim_step)


        # --- C. åŠ¨æ€è®­ç»ƒé˜¶æ®µ (Training) ---
        # æ ¸å¿ƒé€»è¾‘ï¼šåŸºäºæ•°æ®é‡åŠ¨æ€è®¡ç®—è®­ç»ƒæ­¥æ•°
        train_steps = (len(expert_raw_data) * args.epochs_per_ei) // args.batch_size
        train_steps = max(1, train_steps)
        
        print(f">> æ­£åœ¨å¯¹æ–°æ•°æ®è¿›è¡Œé¢„åˆ†è¯...")
        tokenized_expert_data = tokenize_prompt_and_output(
            [ex['prompt'] for ex in expert_raw_data], [ex['response'] for ex in expert_raw_data], tokenizer
        )

        print(f">> å¯åŠ¨ SFT è®­ç»ƒ: æ‰§è¡Œ {train_steps} æ­¥æ›´æ–° (ç­‰æ•ˆ {args.epochs_per_ei} Epochs)...")
        policy.train()
        pbar = tqdm(range(train_steps), desc=f"EI-{ei_step+1} Training")
        
        for _ in pbar:
            acc_loss, acc_glob_ent, acc_res_ent = 0, 0, 0
            
            for _ in range(grad_accum_steps):
                batch = get_batch(tokenized_expert_data, args.micro_batch_size, args.device)
                
                with amp_ctx:
                    logits = policy(batch["input_ids"]).logits
                    
                    # æ˜¾å­˜ä¼˜åŒ– Log-Prob
                    lse = torch.logsumexp(logits, dim=-1)
                    target_logits = torch.gather(logits, -1, batch["labels"].unsqueeze(-1)).squeeze(-1)
                    log_probs = target_logits - lse

                    # ç†µè®¡ç®—
                    with torch.no_grad():
                        ent = compute_entropy(logits)
                        v_mask = (batch["labels"] != tokenizer.pad_token_id)
                        res_mask_bool = batch["response_mask"].bool() & v_mask
                        avg_res_ent = ent[res_mask_bool].mean().item() if res_mask_bool.any() else 0.0
                        avg_glob_ent = ent[v_mask].mean().item()

                    # æ‰§è¡Œå¾®æ‰¹æ¬¡è®­ç»ƒæ­¥ (å†…å« backward)
                    loss, _ = sft_microbatch_train_step(
                        policy_log_probs=log_probs,
                        response_mask=batch["response_mask"],
                        gradient_accumulation_steps=grad_accum_steps,
                        normalize_constant=1.0
                    )
                    acc_loss += loss.item() * grad_accum_steps
                    acc_glob_ent += avg_glob_ent
                    acc_res_ent += avg_res_ent

            # å‚æ•°æ›´æ–°
            torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)
            optimizer.step()
            optimizer.zero_grad()
            global_optim_step += 1
            
            # æ—¥å¿—è®°å½•
            if global_optim_step % 5 == 0:
                wandb.log({
                    "train/loss": acc_loss / grad_accum_steps,
                    "train/global_entropy": acc_glob_ent / grad_accum_steps,
                    "train/response_entropy": acc_res_ent / grad_accum_steps,
                    "global_step": global_optim_step
                })

            if global_optim_step % args.eval_every_steps == 0:
                print(f"\n[Step {global_optim_step}] è®­ç»ƒä¸­é€”è¯„ä¼°...")
                policy.eval()
                # å¿…é¡»åŒæ­¥æƒé‡ï¼Œå¦åˆ™ vLLM è¿˜åœ¨ç”¨æœ¬è½®è®­ç»ƒå¼€å§‹å‰çš„æ—§å‚æ•°
                load_policy_into_vllm_instance(policy, vllm_inst)
                
                metrics = log_generations(
                    vllm_inst, eval_params, 
                    [s['prompt'] for s in val_samples], 
                    [s['gold'] for s in val_samples], 
                    reward_fn, 
                    global_optim_step, 
                    "eval"
                )
                policy.train() # åˆ‡å›è®­ç»ƒæ¨¡å¼

        # --- D. è¿­ä»£åè¯„ä¼° ---
        print(f">> æ­£åœ¨è¿›è¡Œæœ¬è½®è¿­ä»£çš„éªŒè¯...")
        policy.eval()
        load_policy_into_vllm_instance(policy, vllm_inst)
        metrics = log_generations(vllm_inst, eval_params, 
                                 [s['prompt'] for s in val_samples], 
                                 [s['gold'] for s in val_samples], 
                                 reward_fn, global_optim_step, "eval")
        print(f"Accuracy: {metrics.get('eval/accuracy', 0):.2%}")

        # æ¯è½®è¿­ä»£ä¿å­˜ä¸€æ¬¡ Checkpoint
        save_path = os.path.join(args.output_dir, f"ei_iter{ei_step+1}_step{global_optim_step}")
        policy.save_pretrained(save_path)
        tokenizer.save_pretrained(save_path)
        
        # æ¸…ç†æ˜¾å­˜ç¢ç‰‡
        torch.cuda.empty_cache()

    print("\nğŸ‰ Expert Iteration ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼")
    wandb.finish()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="CS336 Expert Iteration Dynamic Step Training")
    
    # è·¯å¾„é…ç½®
    parser.add_argument("--model_id", type=str, default="model/Qwen2.5-Math-1.5B")
    parser.add_argument("--train_data_path", type=str, default="data/gsm8k/train_sft_reason_gsm8k_raw.jsonl")
    parser.add_argument("--val_data_path", type=str, default="data/gsm8k/test.jsonl")
    parser.add_argument("--prompt_path", type=str, default="cs336_alignment/prompts/r1_zero.prompt")
    parser.add_argument("--output_dir", type=str, default="result/ei_checkpoints")
    
    # åŸºç¡€è®­ç»ƒå‚æ•°
    parser.add_argument("--lr", type=float, default=1e-5)
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--micro_batch_size", type=int, default=1)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--max_tokens", type=int, default=1024)
    
    # EI åŠ¨æ€å‚æ•°
    parser.add_argument("--n_ei_steps", type=int, default=5, help="å¤–å±‚è¿­ä»£è½®æ•°")
    parser.add_argument("--ei_batch_size", type=int, default=512, help="æ¯ä¸€è½®é‡‡æ ·çš„é¢˜ç›®æ•° Db")
    parser.add_argument("--rollouts", type=int, default=8, help="æ¯ä¸€é¢˜ç”Ÿæˆçš„å€™é€‰è·¯å¾„æ•° G")
    parser.add_argument("--epochs_per_ei", type=int, default=1, help="æ¯ä¸€è½®å¯¹æ–°ä¸“å®¶æ•°æ®è®­ç»ƒçš„æ¬¡æ•°")
    
    # ç¡¬ä»¶ä¸ç›‘æ§
    parser.add_argument("--device", type=str, default="cuda:0")
    parser.add_argument("--vllm_device", type=str, default="cuda:1")
    parser.add_argument("--vllm_gpu_util", type=float, default=0.2)
    parser.add_argument("--max_eval_samples", type=int, default=100)
    parser.add_argument("--wandb_project", type=str, default="cs336-ei-dynamic")
    parser.add_argument("--wandb_run_name", type=str, default=None)
    parser.add_argument("--eval_every_steps", type=int, default=4)

    args = parser.parse_args()
    run_expert_iteration(args)